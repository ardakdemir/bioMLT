{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_bioner(file):\n",
    "    suff = \"-ENT\"\n",
    "    f = open(file).readlines()\n",
    "    s = \"\"\n",
    "    for l in f:\n",
    "        if l==\"\\n\":\n",
    "            s+=l\n",
    "        else:\n",
    "            x = l\n",
    "            s+=\"{}\\t{}\\n\".format(x.split()[0],x.split()[1]+suff if x.split()[1]!=\"O\" else \"O\")\n",
    "    with open(\"ent_\"+file,\"w\") as x:\n",
    "        x.write(s)\n",
    "annotate_bioner('toy_ner_train.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert2toks = torch.tensor([[0,1,1,2,3,4,4,5,7,7,7],[0,1,1,2,3,4,4,4,5,5,6]])\n",
    "predictions = [[0,1,1,2,3,4,4,5,7,7,7],[0,1,1,2,3,4,4,5]]\n",
    "def _get_token_to_bert_predictions(predictions, bert2toks):\n",
    "    bert_predictions = []\n",
    "    for pred,b2t in zip(predictions,bert2toks):\n",
    "        bert_preds = []\n",
    "        for b in b2t:\n",
    "            bert_preds.append(pred[b])\n",
    "        bert_predictions.append(bert_preds)\n",
    "    return bert_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 1, 1, 2, 3, 3, 4, 5, 5, 5], [0, 1, 1, 1, 2, 3, 3, 3, 4, 4, 4]]\n",
      "[[0, 1, 1, 2, 3, 4, 4, 5, 7, 7, 7], [0, 1, 1, 2, 3, 4, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "bert_preds = _get_token_to_bert_predictions(predictions, bert2toks)\n",
    "print(bert_preds)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"x/y/z.txt\"\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-11fe84d69d3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor([1,2,3,4]).clamp_(2.2,3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = numpy.array([0,1,2,3,4])\n",
    "a[[1,2]] =  [4,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, 3, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ardaakdemir/bioMLT/tokenization.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['====', 'Front'],\n",
       " ['BMC',\n",
       "  'BioinformaticsBMC',\n",
       "  'BioinformaticsBMC',\n",
       "  'Bioinformatics1471-2105BioMed',\n",
       "  'Central',\n",
       "  'London',\n",
       "  '332910.1186/s12859-019-3329-9Methodology',\n",
       "  'ArticleAutomatic',\n",
       "  'construction',\n",
       "  'of',\n",
       "  'metabolic',\n",
       "  'models',\n",
       "  'with',\n",
       "  'enzyme',\n",
       "  'constraints',\n",
       "  'Bekiaris',\n",
       "  'Pavlos',\n",
       "  'Stephanos',\n",
       "  'http://orcid.org/0000-0003-2563-7561Klamt',\n",
       "  'Steffen',\n",
       "  'klamt@mpi-magdeburg.mpg.de',\n",
       "  '0000',\n",
       "  '0004',\n",
       "  '0491',\n",
       "  '802Xgrid.419517.fMax',\n",
       "  'Planck',\n",
       "  'Institute',\n",
       "  'for',\n",
       "  'Dynamics',\n",
       "  'of',\n",
       "  'Complex',\n",
       "  'Technical',\n",
       "  'Systems,',\n",
       "  'Sandtorstr.',\n",
       "  '1,',\n",
       "  'Magdeburg,',\n",
       "  'Germany',\n",
       "  '14',\n",
       "  '1',\n",
       "  '2020',\n",
       "  '14',\n",
       "  '1',\n",
       "  '2020',\n",
       "  '2020',\n",
       "  '21',\n",
       "  '1916',\n",
       "  '9',\n",
       "  '2019',\n",
       "  '17',\n",
       "  '12',\n",
       "  '2019',\n",
       "  'Â©',\n",
       "  'The',\n",
       "  'Author(s).',\n",
       "  '2020Open',\n",
       "  'AccessThis',\n",
       "  'article',\n",
       "  'is',\n",
       "  'distributed',\n",
       "  'under',\n",
       "  'the',\n",
       "  'terms',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Creative',\n",
       "  'Commons',\n",
       "  'Attribution',\n",
       "  '4.0',\n",
       "  'International',\n",
       "  'License',\n",
       "  '(http://creativecommons.org/licenses/by/4.0/),',\n",
       "  'which',\n",
       "  'permits',\n",
       "  'unrestricted',\n",
       "  'use,',\n",
       "  'distribution,',\n",
       "  'and',\n",
       "  'reproduction',\n",
       "  'in',\n",
       "  'any',\n",
       "  'medium,',\n",
       "  'provided',\n",
       "  'you',\n",
       "  'give',\n",
       "  'appropriate',\n",
       "  'credit',\n",
       "  'to',\n",
       "  'the',\n",
       "  'original',\n",
       "  'author(s)',\n",
       "  'and',\n",
       "  'the',\n",
       "  'source,',\n",
       "  'provide',\n",
       "  'a',\n",
       "  'link',\n",
       "  'to',\n",
       "  'the',\n",
       "  'Creative',\n",
       "  'Commons',\n",
       "  'license,',\n",
       "  'and',\n",
       "  'indicate',\n",
       "  'if',\n",
       "  'changes',\n",
       "  'were',\n",
       "  'made.',\n",
       "  'The',\n",
       "  'Creative',\n",
       "  'Commons',\n",
       "  'Public',\n",
       "  'Domain',\n",
       "  'Dedication',\n",
       "  'waiver',\n",
       "  '(http://creativecommons.org/publicdomain/zero/1.0/)',\n",
       "  'applies',\n",
       "  'to',\n",
       "  'the',\n",
       "  'data',\n",
       "  'made',\n",
       "  'available',\n",
       "  'in',\n",
       "  'this',\n",
       "  'article,',\n",
       "  'unless',\n",
       "  'otherwise',\n",
       "  'stated.Background'],\n",
       " ['In',\n",
       "  'order',\n",
       "  'to',\n",
       "  'improve',\n",
       "  'the',\n",
       "  'accuracy',\n",
       "  'of',\n",
       "  'constraint-based',\n",
       "  'metabolic',\n",
       "  'models,',\n",
       "  'several',\n",
       "  'approaches',\n",
       "  'have',\n",
       "  'been',\n",
       "  'developed',\n",
       "  'which',\n",
       "  'intend',\n",
       "  'to',\n",
       "  'integrate',\n",
       "  'additional',\n",
       "  'biological',\n",
       "  'information.',\n",
       "  'Two',\n",
       "  'of',\n",
       "  'these',\n",
       "  'methods,',\n",
       "  'MOMENT',\n",
       "  'and',\n",
       "  'GECKO,',\n",
       "  'incorporate',\n",
       "  'enzymatic',\n",
       "  '(kcat)',\n",
       "  'parameters',\n",
       "  'and',\n",
       "  'enzyme',\n",
       "  'mass',\n",
       "  'constraints',\n",
       "  'to',\n",
       "  'further',\n",
       "  'constrain',\n",
       "  'the',\n",
       "  'space',\n",
       "  'of',\n",
       "  'feasible',\n",
       "  'metabolic',\n",
       "  'flux',\n",
       "  'distributions.',\n",
       "  'While',\n",
       "  'both',\n",
       "  'methods',\n",
       "  'have',\n",
       "  'been',\n",
       "  'proven',\n",
       "  'to',\n",
       "  'deliver',\n",
       "  'useful',\n",
       "  'extensions',\n",
       "  'of',\n",
       "  'metabolic',\n",
       "  'models,',\n",
       "  'they',\n",
       "  'may',\n",
       "  'considerably',\n",
       "  'increase',\n",
       "  'size',\n",
       "  'and',\n",
       "  'complexity',\n",
       "  'of',\n",
       "  'the',\n",
       "  'models',\n",
       "  'and',\n",
       "  'there',\n",
       "  'is',\n",
       "  'currently',\n",
       "  'no',\n",
       "  'tool',\n",
       "  'available',\n",
       "  'to',\n",
       "  'fully',\n",
       "  'automate',\n",
       "  'generation',\n",
       "  'and',\n",
       "  'calibration',\n",
       "  'of',\n",
       "  'such',\n",
       "  'enzyme-constrained',\n",
       "  'models',\n",
       "  'from',\n",
       "  'given',\n",
       "  'stoichiometric',\n",
       "  'models.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenization\n",
    "print(tokenization.__file__)\n",
    "all_documents = [[]]\n",
    "class Tokenizer():\n",
    "    def __init__(self):\n",
    "        self.name = \"toy\"\n",
    "    def tokenize(self,line):\n",
    "        return line.strip().split()\n",
    "    \n",
    "tokenizer= Tokenizer()\n",
    "with open(\"PMC6961255.txt\",'r') as file:\n",
    "    while True:\n",
    "            line = tokenization.convert_to_unicode(file.readline())\n",
    "            if not line:\n",
    "                break\n",
    "            line = line.strip()\n",
    "\n",
    "            # Empty lines are used as document delimiters\n",
    "            if not line:\n",
    "                all_documents.append([])\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            if tokens:\n",
    "                all_documents[-1].append(tokens)\n",
    "all_documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'enumerate' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f63c72ea2563>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'enumerate' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate([1,2,3,4])[:2]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
