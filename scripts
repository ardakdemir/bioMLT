python run_lm_finetuning.py --train_data_file tests_samples/PMC6961255.txt --output_dir save_dir --model_type bert --mlm --tokenizer_name bert-base-cased --model_name_or_path bert-base-cased



BIOMLT Predict script
python biomlt.py --predict --squad_predict_file ~/biobert_data/datasets/QA/BioASQ/BioASQ-test-factoid-6b-1.json --load_model_path save_dir/qas_2_9_9_50


qsub biomlt_predict_submit.sh save_dir/qas_3_9_11_49 ~/biobert_data/datasets/QA/BioASQ/BioASQ-test-factoid-6b-1.json nbest_out

qsub biomlt_train_predict.sh train_predict_save1 ../biobert_data/datasets/QA/BioASQ/BioASQ-test-factoid-6b-1.json


ToY example scripts!!


!!toy yes-no ( use overwrite_cache and squad_yes_no flags!!)

python biomlt.py --mode joint_flat --squad_train_file toy_yesno.json --init_bert --ner_train_file ent_toy_ner_train.tsv --squad_dir . --batch_size 1 --max_seq_length 20 --squad_predict_file toy_qas.json  --squad_yes_no --overwrite_cache



!!toy train_qas (I would like to try loading model parameters to a larger model)

python biomlt.py --mode qas --squad_train_file toy_qas.json --init_bert--squad_dir . --batch_size 1 --max_seq_length 20 --squad_predict_file toy_qas.json


!! toy train load previous model  ! ! load_all_model works!!

python biomlt.py --mode qas --squad_train_file toy_qas.json --init_bert --squad_dir . --batch_size 1 --max_seq_length 20 --squad_predict_file toy_qas.json  --load_model_path save_dir/qas_2_16_3_42 --load_model

python biomlt.py --mode qas --squad_train_file toy_qas.json  --squad_dir . --batch_size 12  --squad_predict_file toy_qas.json  --init_bert




## Train on 8b test on 7b-trainn

python biomlt_alldata.py --load_model_path key_models/mybiobert_finetunedsquad --model_save_name alldata_8b_0403 --num_train_epochs --squad_train_factoid_file ~/biobert_data/BioASQ-training8b/training8b_squadformat_train_factoid.json --squad_train_list_file ~/biobert_data/BioASQ-training8b/training8b_squadformat_train_list.json --squad_train_yesno_file ~/biobert_data/BioASQ-training8b/training8b_squadformat_train_yesno.json --squad_predict_factoid_file ~/biobert_data/BioASQ-7b/train/Snippet-as-is/train_factoid_7b_snippet.json --squad_predict_list_file ~/biobert_data/BioASQ-7b/train/Snippet-as-is/train_list_7b_snippet.json --squad_predict_yesno_file ~/biobert_data/BioASQ-7b/train/Snippet-as-is/train_yesno_7b_snippet.json --max_seq_length 256



## Train qas model with a ner head (without joint learning)
python bioMLT/biomlt_alldata.py --mode qas  --num_train_epochs 10 --total_train_steps 1000  --output_dir qasonly_1311_full_test8b --max_seq_length 256 --batch_size 12 --load_model


## Train qas starting with ner model with a ner head of course!
python bioMLT/biomlt_alldata.py --mode qas  --num_train_epochs 2 --total_train_steps 100  --crf --init_ner_head --model_save_name qasmodel1111  --output_dir out1111_loadnerhead  --max_seq_length 256 --batch_size 12 --load_model_path neronly1111_deneme/best_ner_model_on_BC2GM


## Train ner only model with qas head initialized
python bioMLT/biomlt_alldata.py --mode ner --crf --ner_train_file biobert_data/datasets/NER/BC2GM/ent_train.tsv --ner_test_file biobert_data/datasets/NER/BC2GM/ent_test.tsv --ner_dev_file biobert_data/datasets/NER/BC2GM/ent_devel.tsv --num_train_epochs 10 --total_train_steps 100 --model_save_name nermodel0911  --output_dir neronly1111_deneme --batch_size 12 --load_model



## Qas with ner head input hierarchical
## This code gives the embedding of label predictions of ner to qas component

python bioMLT/biomlt_alldata.py --mode qas  --num_train_epochs 2 --total_train_steps 100  --crf --init_ner_head --model_save_name qasmodel1111  --output_dir out1111_loadnerhead  --max_seq_length 256 --batch_size 12 --load_model_path neronly_stls_0811_folder/best_ner_model_aux__target_BC2GM   --load_ner_label_vocab_path best_ner_model_on_BC2GM_vocab --qas_with_ner

python bioMLT/biomlt_alldata.py --mode qas  --num_train_epochs 2 --total_train_steps 100  --crf --init_ner_head --output_dir out1511_deneme  --max_seq_length 256 --batch_size 12 --load_model_path neronly_outs_1311/best_ner_model_on_BC2GM     --load_ner_label_vocab_path  neronly_outs_1311/best_ner_model_on_BC2GM_vocab --qas_with_ner




singularity exec --nv ~/singularity/pt-cuda-tf python bioMLT/biomlt_alldata.py --mode qas --output_dir $output_dir  --load_model_path ${load_model_path} --load_ner_label_vocab_path ${load_ner_vocab_path} --max_seq_length 256 --num_train_epochs 30  --qas_with_ner --init_ner_head --crf


# Qas hier all submitter
bash bioMLT/qashier_all_submitter.sh qashier1711_0825 neronly_outs_1311 qashier1711_1
